{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss, roc_auc_score, auc, roc_curve\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss, roc_auc_score, auc, roc_curve\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_train = pd.read_csv('ccf_offline_stage1_train.csv')\n",
    "off_train.columns = ['user_id','merchant_id','coupon_id','discount_rate','distance','date_received','date']\n",
    "#2050 coupon_id. date_received:20160701~20160731, 76309 users(76307 in trainset, 35965 in online_trainset), 1559 merchants(1558 in trainset)\n",
    "off_test = pd.read_csv('ccf_offline_stage1_test_revised.csv')\n",
    "off_test.columns = ['user_id','merchant_id','coupon_id','discount_rate','distance','date_received']\n",
    "#11429826 record(872357 with coupon_id),762858 user(267448 in off_train)\n",
    "on_train = pd.read_csv('ccf_online_stage1_train.csv')\n",
    "on_train.columns = ['user_id','merchant_id','action','coupon_id','discount_rate','date_received','date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_train[['date','date_received']] = off_train[['date','date_received']].astype(str)\n",
    "off_train[['date','date_received']] = off_train[['date','date_received']].replace('nan','null')\n",
    "\n",
    "off_test[['date_received']] = off_test[['date_received']].astype(str)\n",
    "off_test[['date_received']] = off_test[['date_received']].replace('nan','null')\n",
    "\n",
    "on_train[['date','date_received']] = on_train[['date','date_received']].astype(str)\n",
    "on_train[['date','date_received']] = on_train[['date','date_received']].replace('nan','null')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = off_test\n",
    "feature3 = off_train[((off_train.date>='20160315')&(off_train.date<='20160630'))|((off_train.date=='null')&(off_train.date_received>='20160315')&(off_train.date_received<='20160630'))]\n",
    "dataset2 = off_train[(off_train.date_received>='20160515')&(off_train.date_received<='20160615')]\n",
    "feature2 = off_train[(off_train.date>='20160201')&(off_train.date<='20160514')|((off_train.date=='null')&(off_train.date_received>='20160201')&(off_train.date_received<='20160514'))]\n",
    "dataset1 = off_train[(off_train.date_received>='20160414')&(off_train.date_received<='20160514')]\n",
    "feature1 = off_train[(off_train.date>='20160101')&(off_train.date<='20160413')|((off_train.date=='null')&(off_train.date_received>='20160101')&(off_train.date_received<='20160413'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_id  coupon_id                        date_received  receive_number\n",
      "6        448      10927  20160710:20160707:20160706:20160704               4\n",
      "13       736       3686                    20160713:20160724               2\n",
      "24      1318      11894                    20160728:20160720               2\n",
      "128     9208       6335                    20160716:20160703               2\n",
      "155    11170      11873  20160717:20160715:20160724:20160730               4\n",
      "207    14321       2111           20160716:20160712:20160730               3\n",
      "210    14360       5035           20160726:20160718:20160729               3\n",
      "219    14916       2197                    20160723:20160721               2\n",
      "262    17589      10141  20160714:20160717:20160729:20160702               4\n",
      "293    19732       9639                    20160722:20160718               2\n",
      "324    21564       3944           20160713:20160714:20160715               3\n",
      "375    25208      11894                    20160716:20160714               2\n",
      "406    27134      12061                    20160709:20160715               2\n",
      "528    36397       2435           20160721:20160720:20160723               3\n",
      "584    40900      13267           20160730:20160723:20160730               3\n",
      "644    46054      13164                    20160715:20160716               2\n",
      "663    47130       2111                    20160731:20160722               2\n",
      "667    47166      10887           20160726:20160726:20160727               3\n",
      "681    48772       1137                    20160724:20160710               2\n",
      "737    52816      10028  20160704:20160705:20160708:20160712               4\n",
      "746    53331       2357                    20160721:20160722               2\n",
      "809    57413      11535                    20160718:20160705               2\n",
      "810    57607       2693                    20160723:20160731               2\n",
      "813    57920       4727                    20160705:20160724               2\n",
      "870    60984       5412           20160719:20160715:20160704               3\n",
      "891    62354       6500                    20160701:20160706               2\n",
      "924    64163      12239           20160706:20160705:20160707               3\n",
      "960    66842       1699           20160722:20160720:20160716               3\n",
      "961    66842      12229           20160722:20160716:20160720               3\n",
      "969    68038        864                    20160722:20160727               2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "t = dataset3[['user_id']]\n",
    "t['this_month_user_receive_all_coupon_count'] = 1\n",
    "t = t.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t1 = dataset3[['user_id','coupon_id']]\n",
    "t1['this_month_user_receive_same_coupon_count'] = 1\n",
    "t1 = t1.groupby(['user_id','coupon_id']).agg('sum').reset_index()\n",
    "\n",
    "t2 = dataset3[['user_id','coupon_id','date_received']]\n",
    "t2 = t2.replace('null',np.nan).dropna() #自己加上去的，看看会不会有影响\n",
    "t2.date_received = t2.date_received.astype('str')\n",
    "t2 = t2.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t2['receive_number'] = t2.date_received.apply(lambda s:len(s.split(':')))\n",
    "t2 = t2[t2.receive_number>1]\n",
    "print(t2.head(30))\n",
    "t2['max_date_received'] = t2.date_received.apply(lambda s:max([float(d)  for d in s.split(':') if d!='null']))\n",
    "t2['min_date_received'] = t2.date_received.apply(lambda s:min([float(d) for d in s.split(':') if d!='null']))\n",
    "t2 = t2[['user_id','coupon_id','max_date_received','min_date_received']]\n",
    "\n",
    "t3 = dataset3[['user_id','coupon_id','date_received']]\n",
    "t3 = pd.merge(t3,t2,on=['user_id','coupon_id'],how='left')\n",
    "t3['this_month_user_receive_same_coupon_lastone'] = t3.max_date_received - t3.date_received.replace('null',np.nan).astype(float)\n",
    "t3['this_month_user_receive_same_coupon_firstone'] = t3.date_received.replace('null',np.nan).astype(float) - t3.min_date_received\n",
    "def is_firstlastone(x):\n",
    "    if x==0:\n",
    "        return 1\n",
    "    elif x>0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 #those only receive once\n",
    "        \n",
    "t3.this_month_user_receive_same_coupon_lastone = t3.this_month_user_receive_same_coupon_lastone.apply(is_firstlastone)\n",
    "t3.this_month_user_receive_same_coupon_firstone = t3.this_month_user_receive_same_coupon_firstone.apply(is_firstlastone)\n",
    "t3 = t3[['user_id','coupon_id','date_received','this_month_user_receive_same_coupon_lastone','this_month_user_receive_same_coupon_firstone']]\n",
    "\n",
    "t4 = dataset3[['user_id','date_received']]\n",
    "t4['this_day_user_receive_all_coupon_count'] = 1\n",
    "t4 = t4.groupby(['user_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t5 = dataset3[['user_id','coupon_id','date_received']]\n",
    "t5['this_day_user_receive_same_coupon_count'] = 1\n",
    "t5 = t5.groupby(['user_id','coupon_id','date_received']).agg('sum').reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_gap_before(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        if date_received != 'null' and d != 'null':\n",
    "            this_gap = (date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))-date(int(d[0:4]),int(d[4:6]),int(d[6:8]))).days\n",
    "            if this_gap>0:\n",
    "                gaps.append(this_gap)\n",
    "        else:\n",
    "            this_gap = 0\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "        \n",
    "def get_day_gap_after(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        if date_received != 'null' and d != 'null':\n",
    "            this_gap = (date(int(d[0:4]),int(d[4:6]),int(d[6:8]))-date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))).days\n",
    "            if this_gap>0:\n",
    "                gaps.append(this_gap)\n",
    "        else:\n",
    "            this_gap = 0\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3[['date_received']]=dataset3[['date_received']].replace(np.nan,'null')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5096: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116204, 11)\n"
     ]
    }
   ],
   "source": [
    "t6 = dataset3[['user_id','coupon_id','date_received']]\n",
    "t6.date_received = t6.date_received.astype('str')\n",
    "t6 = t6.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t6.rename(columns={'date_received':'dates'},inplace=True)\n",
    "\n",
    "t7 = dataset3[['user_id','coupon_id','date_received']]\n",
    "t7 = pd.merge(t7,t6,on=['user_id','coupon_id'],how='left')\n",
    "t7['date_received_date'] = t7.date_received.astype('str') + '-' + t7.dates\n",
    "t7['day_gap_before'] = t7.date_received_date.apply(get_day_gap_before)\n",
    "t7['day_gap_after'] = t7.date_received_date.apply(get_day_gap_after)\n",
    "t7 = t7[['user_id','coupon_id','date_received','day_gap_before','day_gap_after']]\n",
    "\n",
    "other_feature3 = pd.merge(t1,t,on='user_id')\n",
    "other_feature3 = pd.merge(other_feature3,t3,on=['user_id','coupon_id'])\n",
    "other_feature3 = pd.merge(other_feature3,t4,on=['user_id','date_received'])\n",
    "other_feature3 = pd.merge(other_feature3,t5,on=['user_id','coupon_id','date_received'])\n",
    "other_feature3 = pd.merge(other_feature3,t7,on=['user_id','coupon_id','date_received'])\n",
    "other_feature3.to_csv('other_feature3.csv',index=None)\n",
    "print(other_feature3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#for dataset2\n",
    "t = dataset2[['user_id']]\n",
    "t['this_month_user_receive_all_coupon_count'] = 1\n",
    "t = t.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t1 = dataset2[['user_id','coupon_id']]\n",
    "t1['this_month_user_receive_same_coupon_count'] = 1\n",
    "t1 = t1.groupby(['user_id','coupon_id']).agg('sum').reset_index()\n",
    "\n",
    "t2 = dataset2[['user_id','coupon_id','date_received']]\n",
    "t2 = t2.replace('null',np.nan).dropna() #自己加上去的，看看会不会有影响\n",
    "t2.date_received = t2.date_received.astype('str')\n",
    "t2 = t2.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t2['receive_number'] = t2.date_received.apply(lambda s:len(s.split(':')))\n",
    "t2 = t2[t2.receive_number>1]\n",
    "t2['max_date_received'] = t2.date_received.apply(lambda s:max([float(d)  for d in s.split(':') if d!='null']))\n",
    "t2['min_date_received'] = t2.date_received.apply(lambda s:min([float(d)  for d in s.split(':') if d!='null']))\n",
    "t2 = t2[['user_id','coupon_id','max_date_received','min_date_received']]\n",
    "\n",
    "t3 = dataset2[['user_id','coupon_id','date_received']]\n",
    "t3 = pd.merge(t3,t2,on=['user_id','coupon_id'],how='left')\n",
    "t3['this_month_user_receive_same_coupon_lastone'] = t3.max_date_received - t3.date_received.replace('null',np.nan).astype(float)\n",
    "t3['this_month_user_receive_same_coupon_firstone'] = t3.date_received.replace('null',np.nan).astype(float) - t3.min_date_received\n",
    "def is_firstlastone(x):\n",
    "    if x==0:\n",
    "        return 1\n",
    "    elif x>0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 #those only receive once\n",
    "        \n",
    "t3.this_month_user_receive_same_coupon_lastone = t3.this_month_user_receive_same_coupon_lastone.apply(is_firstlastone)\n",
    "t3.this_month_user_receive_same_coupon_firstone = t3.this_month_user_receive_same_coupon_firstone.apply(is_firstlastone)\n",
    "t3 = t3[['user_id','coupon_id','date_received','this_month_user_receive_same_coupon_lastone','this_month_user_receive_same_coupon_firstone']]\n",
    "\n",
    "t4 = dataset2[['user_id','date_received']]\n",
    "t4['this_day_user_receive_all_coupon_count'] = 1\n",
    "t4 = t4.groupby(['user_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t5 = dataset2[['user_id','coupon_id','date_received']]\n",
    "t5['this_day_user_receive_same_coupon_count'] = 1\n",
    "t5 = t5.groupby(['user_id','coupon_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t6 = dataset2[['user_id','coupon_id','date_received']]\n",
    "t6.date_received = t6.date_received.astype('str')\n",
    "t6 = t6.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t6.rename(columns={'date_received':'dates'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_gap_before(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        if date_received != 'null' and d != 'null':\n",
    "            this_gap = (date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))-date(int(d[0:4]),int(d[4:6]),int(d[6:8]))).days\n",
    "            if this_gap>0:\n",
    "                gaps.append(this_gap)\n",
    "        else:\n",
    "            this_gap = 0\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "        \n",
    "def get_day_gap_after(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        if date_received != 'null' and d != 'null':\n",
    "            this_gap = (date(int(d[0:4]),int(d[4:6]),int(d[6:8]))-date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))).days\n",
    "            if this_gap>0:\n",
    "                gaps.append(this_gap)\n",
    "        else:\n",
    "            this_gap = 0\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(258705, 11)\n"
     ]
    }
   ],
   "source": [
    "t7 = dataset2[['user_id','coupon_id','date_received']]\n",
    "t7 = pd.merge(t7,t6,on=['user_id','coupon_id'],how='left')\n",
    "t7['date_received_date'] = t7.date_received.astype('str') + '-' + t7.dates\n",
    "t7['day_gap_before'] = t7.date_received_date.apply(get_day_gap_before)\n",
    "t7['day_gap_after'] = t7.date_received_date.apply(get_day_gap_after)\n",
    "t7 = t7[['user_id','coupon_id','date_received','day_gap_before','day_gap_after']]\n",
    "\n",
    "other_feature2 = pd.merge(t1,t,on='user_id')\n",
    "other_feature2 = pd.merge(other_feature2,t3,on=['user_id','coupon_id'])\n",
    "other_feature2 = pd.merge(other_feature2,t4,on=['user_id','date_received'])\n",
    "other_feature2 = pd.merge(other_feature2,t5,on=['user_id','coupon_id','date_received'])\n",
    "other_feature2 = pd.merge(other_feature2,t7,on=['user_id','coupon_id','date_received'])\n",
    "other_feature2.to_csv('other_feature2.csv',index=None)\n",
    "print(other_feature2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#for dataset1\n",
    "t = dataset1[['user_id']]\n",
    "t['this_month_user_receive_all_coupon_count'] = 1\n",
    "t = t.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t1 = dataset1[['user_id','coupon_id']]\n",
    "t1['this_month_user_receive_same_coupon_count'] = 1\n",
    "t1 = t1.groupby(['user_id','coupon_id']).agg('sum').reset_index()\n",
    "\n",
    "t2 = dataset1[['user_id','coupon_id','date_received']]\n",
    "t2 = t2.replace('null',np.nan).dropna() #自己加上去的，看看会不会有影响\n",
    "t2.date_received = t2.date_received.astype('str')\n",
    "t2 = t2.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t2['receive_number'] = t2.date_received.apply(lambda s:len(s.split(':')))\n",
    "t2 = t2[t2.receive_number>1]\n",
    "t2['max_date_received'] = t2.date_received.apply(lambda s:max([float(d)  for d in s.split(':') if d!='null']))\n",
    "t2['min_date_received'] = t2.date_received.apply(lambda s:min([float(d)  for d in s.split(':') if d!='null']))\n",
    "t2 = t2[['user_id','coupon_id','max_date_received','min_date_received']]\n",
    "\n",
    "t3 = dataset1[['user_id','coupon_id','date_received']]\n",
    "t3 = pd.merge(t3,t2,on=['user_id','coupon_id'],how='left')\n",
    "t3['this_month_user_receive_same_coupon_lastone'] = t3.max_date_received - t3.date_received.replace('null',np.nan).astype(float)\n",
    "t3['this_month_user_receive_same_coupon_firstone'] = t3.date_received.replace('null',np.nan).astype(float) - t3.min_date_received\n",
    "def is_firstlastone(x):\n",
    "    if x==0:\n",
    "        return 1\n",
    "    elif x>0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 #those only receive once\n",
    "        \n",
    "t3.this_month_user_receive_same_coupon_lastone = t3.this_month_user_receive_same_coupon_lastone.apply(is_firstlastone)\n",
    "t3.this_month_user_receive_same_coupon_firstone = t3.this_month_user_receive_same_coupon_firstone.apply(is_firstlastone)\n",
    "t3 = t3[['user_id','coupon_id','date_received','this_month_user_receive_same_coupon_lastone','this_month_user_receive_same_coupon_firstone']]\n",
    "\n",
    "t4 = dataset1[['user_id','date_received']]\n",
    "t4['this_day_user_receive_all_coupon_count'] = 1\n",
    "t4 = t4.groupby(['user_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t5 = dataset1[['user_id','coupon_id','date_received']]\n",
    "t5['this_day_user_receive_same_coupon_count'] = 1\n",
    "t5 = t5.groupby(['user_id','coupon_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t6 = dataset1[['user_id','coupon_id','date_received']]\n",
    "t6.date_received = t6.date_received.astype('str')\n",
    "t6 = t6.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t6.rename(columns={'date_received':'dates'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_gap_before(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        if date_received != 'null' and d != 'null':\n",
    "            this_gap = (date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))-date(int(d[0:4]),int(d[4:6]),int(d[6:8]))).days\n",
    "            if this_gap>0:\n",
    "                gaps.append(this_gap)\n",
    "        else:\n",
    "            this_gap = 0\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "        \n",
    "def get_day_gap_after(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        if date_received != 'null' and d != 'null':\n",
    "            this_gap = (date(int(d[0:4]),int(d[4:6]),int(d[6:8]))-date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))).days\n",
    "            if this_gap>0:\n",
    "                gaps.append(this_gap)\n",
    "        else:\n",
    "            this_gap = 0\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134232, 11)\n"
     ]
    }
   ],
   "source": [
    "t7 = dataset1[['user_id','coupon_id','date_received']]\n",
    "t7 = pd.merge(t7,t6,on=['user_id','coupon_id'],how='left')\n",
    "t7['date_received_date'] = t7.date_received.astype('str') + '-' + t7.dates\n",
    "t7['day_gap_before'] = t7.date_received_date.apply(get_day_gap_before)\n",
    "t7['day_gap_after'] = t7.date_received_date.apply(get_day_gap_after)\n",
    "t7 = t7[['user_id','coupon_id','date_received','day_gap_before','day_gap_after']]\n",
    "\n",
    "other_feature1 = pd.merge(t1,t,on='user_id')\n",
    "other_feature1 = pd.merge(other_feature1,t3,on=['user_id','coupon_id'])\n",
    "other_feature1 = pd.merge(other_feature1,t4,on=['user_id','date_received'])\n",
    "other_feature1 = pd.merge(other_feature1,t5,on=['user_id','coupon_id','date_received'])\n",
    "other_feature1 = pd.merge(other_feature1,t7,on=['user_id','coupon_id','date_received'])\n",
    "other_feature1.to_csv('other_feature1.csv',index=None)\n",
    "print(other_feature1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_discount_rate(row):\n",
    "    if pd.isnull(row):\n",
    "        return 1.0\n",
    "    elif ':' in str(row):\n",
    "        rows = row.split(':')\n",
    "        return 1.0 - float(rows[1])/float(rows[0])\n",
    "    else:\n",
    "        return float(row)\n",
    "\n",
    "def get_discount_man(row):\n",
    "    if ':' in str(row):\n",
    "        rows = row.split(':')\n",
    "        return int(rows[0])\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "def get_discount_jian(row):\n",
    "    if ':' in str(row):\n",
    "        rows = row.split(':')\n",
    "        return int(rows[1])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def is_man_jian(row):\n",
    "    if pd.isnull(row):\n",
    "        return 0\n",
    "    elif ':' in str(row):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeekday(row):\n",
    "    if row == 'null':\n",
    "        return row\n",
    "    else:\n",
    "        return date(int(row[0:4]), int(row[4:6]), int(row[6:8])).weekday() + 1\n",
    "def getMonth(x):\n",
    "    if x == 'null':\n",
    "        return x\n",
    "    else:\n",
    "        return int(x[6:8])\n",
    "    \n",
    "def getDaydistance3(x):\n",
    "    if x == 'null':\n",
    "        return x\n",
    "    else:\n",
    "        return (date(int(x[0:4]),int(x[4:6]),int(x[6:8]))-date(2016,6,30)).days\n",
    "    \n",
    "def getDaydistance2(x):\n",
    "    if x == 'null':\n",
    "        return x\n",
    "    else:\n",
    "        return (date(int(x[0:4]),int(x[4:6]),int(x[6:8]))-date(2016,5,14)).days\n",
    "\n",
    "def getDaydistance1(x):\n",
    "    if x == 'null':\n",
    "        return x\n",
    "    else:\n",
    "        return (date(int(x[0:4]),int(x[4:6]),int(x[6:8]))-date(2016,4,13)).days\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#dataset3\n",
    "dataset3['day_of_week'] = dataset3.date_received.astype('str').apply(getWeekday)\n",
    "dataset3['day_of_month'] = dataset3.date_received.astype('str').apply(getMonth)\n",
    "dataset3['days_distance'] = dataset3.date_received.astype('str').apply(getDaydistance3)\n",
    "dataset3['discount_man'] = dataset3.discount_rate.apply(get_discount_man)\n",
    "dataset3['discount_jian'] = dataset3.discount_rate.apply(get_discount_jian)\n",
    "dataset3['is_man_jian'] = dataset3.discount_rate.apply(is_man_jian)\n",
    "dataset3['discount_rate'] = dataset3.discount_rate.apply(calc_discount_rate)\n",
    "d = dataset3[['coupon_id']]\n",
    "d['coupon_count'] = 1\n",
    "d = d.groupby('coupon_id').agg('sum').reset_index()\n",
    "dataset3 = pd.merge(dataset3,d,on='coupon_id',how='left')\n",
    "dataset3.to_csv('coupon3_feature.csv',index=None)\n",
    "#dataset2\n",
    "dataset2['day_of_week'] = dataset2.date_received.astype('str').apply(getWeekday)\n",
    "dataset2['day_of_month'] = dataset2.date_received.astype('str').apply(getMonth)\n",
    "dataset2['days_distance'] = dataset2.date_received.astype('str').apply(getDaydistance2)\n",
    "dataset2['discount_man'] = dataset2.discount_rate.apply(get_discount_man)\n",
    "dataset2['discount_jian'] = dataset2.discount_rate.apply(get_discount_jian)\n",
    "dataset2['is_man_jian'] = dataset2.discount_rate.apply(is_man_jian)\n",
    "dataset2['discount_rate'] = dataset2.discount_rate.apply(calc_discount_rate)\n",
    "d = dataset2[['coupon_id']]\n",
    "d['coupon_count'] = 1\n",
    "d = d.groupby('coupon_id').agg('sum').reset_index()\n",
    "dataset2 = pd.merge(dataset2,d,on='coupon_id',how='left')\n",
    "dataset2.to_csv('coupon2_feature.csv',index=None)\n",
    "#dataset1\n",
    "dataset1['day_of_week'] = dataset1.date_received.astype('str').apply(getWeekday)\n",
    "dataset1['day_of_month'] = dataset1.date_received.astype('str').apply(getMonth)\n",
    "dataset1['days_distance'] = dataset1.date_received.astype('str').apply(getDaydistance1)\n",
    "dataset1['discount_man'] = dataset1.discount_rate.apply(get_discount_man)\n",
    "dataset1['discount_jian'] = dataset1.discount_rate.apply(get_discount_jian)\n",
    "dataset1['is_man_jian'] = dataset1.discount_rate.apply(is_man_jian)\n",
    "dataset1['discount_rate'] = dataset1.discount_rate.apply(calc_discount_rate)\n",
    "d = dataset1[['coupon_id']]\n",
    "d['coupon_count'] = 1\n",
    "d = d.groupby('coupon_id').agg('sum').reset_index()\n",
    "dataset1 = pd.merge(dataset1,d,on='coupon_id',how='left')\n",
    "dataset1.to_csv('coupon1_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#for dataset3\n",
    "merchant3 = feature3[['merchant_id','coupon_id','distance','date_received','date']]\n",
    "\n",
    "t = merchant3[['merchant_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = merchant3[merchant3.date!='null'][['merchant_id']]\n",
    "t1['total_sales'] = 1\n",
    "t1 = t1.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t2 = merchant3[(merchant3.date!='null')&(merchant3.coupon_id.notna())][['merchant_id']]\n",
    "t2['sales_use_coupon'] = 1\n",
    "t2 = t2.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t3 = merchant3[merchant3.coupon_id.notna()][['merchant_id']]\n",
    "t3['total_coupon'] = 1\n",
    "t3 = t3.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t4 = merchant3[(merchant3.date!='null')&(merchant3.coupon_id.notna())][['merchant_id','distance']]\n",
    "t4.replace(np.nan,-1,inplace=True)\n",
    "t4.distance = t4.distance.astype('int')\n",
    "t4.replace(-1,np.nan,inplace=True)\n",
    "t5 = t4.groupby('merchant_id').agg('min').reset_index()\n",
    "t5.rename(columns={'distance':'merchant_min_distance'},inplace=True)\n",
    "\n",
    "t6 = t4.groupby('merchant_id').agg('max').reset_index()\n",
    "t6.rename(columns={'distance':'merchant_max_distance'},inplace=True)\n",
    "\n",
    "t7 = t4.groupby('merchant_id').agg('mean').reset_index()\n",
    "t7.rename(columns={'distance':'merchant_mean_distance'},inplace=True)\n",
    "\n",
    "t8 = t4.groupby('merchant_id').agg('median').reset_index()\n",
    "t8.rename(columns={'distance':'merchant_median_distance'},inplace=True)\n",
    "\n",
    "merchant3_feature = pd.merge(t,t1,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t2,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t3,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t5,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t6,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t7,on='merchant_id',how='left')\n",
    "merchant3_feature = pd.merge(merchant3_feature,t8,on='merchant_id',how='left')\n",
    "merchant3_feature.sales_use_coupon = merchant3_feature.sales_use_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant3_feature['merchant_coupon_transfer_rate'] = merchant3_feature.sales_use_coupon.astype('float') / merchant3_feature.total_coupon\n",
    "merchant3_feature['coupon_rate'] = merchant3_feature.sales_use_coupon.astype('float') / merchant3_feature.total_sales\n",
    "merchant3_feature.total_coupon = merchant3_feature.total_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant3_feature.to_csv('merchant3_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#for dataset2\n",
    "merchant2 = feature2[['merchant_id','coupon_id','distance','date_received','date']]\n",
    "\n",
    "t = merchant2[['merchant_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = merchant2[merchant2.date.notna()][['merchant_id']]\n",
    "t1['total_sales'] = 1\n",
    "t1 = t1.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t2 = merchant2[(merchant2.date.notna())&(merchant2.coupon_id.notna())][['merchant_id']]\n",
    "t2['sales_use_coupon'] = 1\n",
    "t2 = t2.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t3 = merchant2[merchant2.coupon_id.notna()][['merchant_id']]\n",
    "t3['total_coupon'] = 1\n",
    "t3 = t3.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t4 = merchant2[(merchant2.date.notna())&(merchant2.coupon_id.notna())][['merchant_id','distance']]\n",
    "t4.replace(np.nan,-1,inplace=True)\n",
    "t4.distance = t4.distance.astype('int')\n",
    "t4.replace(-1,np.nan,inplace=True)\n",
    "t5 = t4.groupby('merchant_id').agg('min').reset_index()\n",
    "t5.rename(columns={'distance':'merchant_min_distance'},inplace=True)\n",
    "\n",
    "t6 = t4.groupby('merchant_id').agg('max').reset_index()\n",
    "t6.rename(columns={'distance':'merchant_max_distance'},inplace=True)\n",
    "\n",
    "t7 = t4.groupby('merchant_id').agg('mean').reset_index()\n",
    "t7.rename(columns={'distance':'merchant_mean_distance'},inplace=True)\n",
    "\n",
    "t8 = t4.groupby('merchant_id').agg('median').reset_index()\n",
    "t8.rename(columns={'distance':'merchant_median_distance'},inplace=True)\n",
    "\n",
    "merchant2_feature = pd.merge(t,t1,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t2,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t3,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t5,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t6,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t7,on='merchant_id',how='left')\n",
    "merchant2_feature = pd.merge(merchant2_feature,t8,on='merchant_id',how='left')\n",
    "merchant2_feature.sales_use_coupon = merchant2_feature.sales_use_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant2_feature['merchant_coupon_transfer_rate'] = merchant2_feature.sales_use_coupon.astype('float') / merchant2_feature.total_coupon\n",
    "merchant2_feature['coupon_rate'] = merchant2_feature.sales_use_coupon.astype('float') / merchant2_feature.total_sales\n",
    "merchant2_feature.total_coupon = merchant2_feature.total_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant2_feature.to_csv('merchant2_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#for dataset1\n",
    "merchant1 = feature1[['merchant_id','coupon_id','distance','date_received','date']]\n",
    "\n",
    "t = merchant1[['merchant_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = merchant1[merchant1.date.notna()][['merchant_id']]\n",
    "t1['total_sales'] = 1\n",
    "t1 = t1.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t2 = merchant1[(merchant1.date.notna())&(merchant1.coupon_id.notna())][['merchant_id']]\n",
    "t2['sales_use_coupon'] = 1\n",
    "t2 = t2.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t3 = merchant1[merchant1.coupon_id.notna()][['merchant_id']]\n",
    "t3['total_coupon'] = 1\n",
    "t3 = t3.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "t4 = merchant1[(merchant1.date.notna())&(merchant1.coupon_id.notna())][['merchant_id','distance']]\n",
    "t4.replace(np.nan,-1,inplace=True)\n",
    "t4.distance = t4.distance.astype('int')\n",
    "t4.replace(-1,np.nan,inplace=True)\n",
    "t5 = t4.groupby('merchant_id').agg('min').reset_index()\n",
    "t5.rename(columns={'distance':'merchant_min_distance'},inplace=True)\n",
    "\n",
    "t6 = t4.groupby('merchant_id').agg('max').reset_index()\n",
    "t6.rename(columns={'distance':'merchant_max_distance'},inplace=True)\n",
    "\n",
    "t7 = t4.groupby('merchant_id').agg('mean').reset_index()\n",
    "t7.rename(columns={'distance':'merchant_mean_distance'},inplace=True)\n",
    "\n",
    "t8 = t4.groupby('merchant_id').agg('median').reset_index()\n",
    "t8.rename(columns={'distance':'merchant_median_distance'},inplace=True)\n",
    "\n",
    "\n",
    "merchant1_feature = pd.merge(t,t1,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t2,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t3,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t5,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t6,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t7,on='merchant_id',how='left')\n",
    "merchant1_feature = pd.merge(merchant1_feature,t8,on='merchant_id',how='left')\n",
    "merchant1_feature.sales_use_coupon = merchant1_feature.sales_use_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant1_feature['merchant_coupon_transfer_rate'] = merchant1_feature.sales_use_coupon.astype('float') / merchant1_feature.total_coupon\n",
    "merchant1_feature['coupon_rate'] = merchant1_feature.sales_use_coupon.astype('float') / merchant1_feature.total_sales\n",
    "merchant1_feature.total_coupon = merchant1_feature.total_coupon.replace(np.nan,0) #fillna with 0\n",
    "merchant1_feature.to_csv('merchant1_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "def get_user_date_datereceived_gap(s):\n",
    "    s = s.split(':')\n",
    "    return (date(int(s[0][0:4]),int(s[0][4:6]),int(s[0][6:8])) - date(int(s[1][0:4]),int(s[1][4:6]),int(s[1][6:8]))).days\n",
    "\n",
    "#for dataset3\n",
    "user3 = feature3[['user_id','merchant_id','coupon_id','discount_rate','distance','date_received','date']]\n",
    "\n",
    "t = user3[['user_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = user3[user3.date!='null'][['user_id','merchant_id']]\n",
    "t1.drop_duplicates(inplace=True)\n",
    "t1.merchant_id = 1\n",
    "t1 = t1.groupby('user_id').agg('sum').reset_index()\n",
    "t1.rename(columns={'merchant_id':'count_merchant'},inplace=True)\n",
    "\n",
    "t2 = user3[(user3.date!='null')&(user3.coupon_id.notna())][['user_id','distance']]\n",
    "t2.replace(np.nan,-1,inplace=True)\n",
    "t2.distance = t2.distance.astype('int')\n",
    "t2.replace(-1,np.nan,inplace=True)\n",
    "t3 = t2.groupby('user_id').agg('min').reset_index()\n",
    "t3.rename(columns={'distance':'user_min_distance'},inplace=True)\n",
    "\n",
    "t4 = t2.groupby('user_id').agg('max').reset_index()\n",
    "t4.rename(columns={'distance':'user_max_distance'},inplace=True)\n",
    "\n",
    "t5 = t2.groupby('user_id').agg('mean').reset_index()\n",
    "t5.rename(columns={'distance':'user_mean_distance'},inplace=True)\n",
    "\n",
    "t6 = t2.groupby('user_id').agg('median').reset_index()\n",
    "t6.rename(columns={'distance':'user_median_distance'},inplace=True)\n",
    "\n",
    "t7 = user3[(user3.date!='null')&(user3.coupon_id.notna())][['user_id']]\n",
    "t7['buy_use_coupon'] = 1\n",
    "t7 = t7.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t8 = user3[user3.date!='null'][['user_id']]\n",
    "t8['buy_total'] = 1\n",
    "t8 = t8.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t9 = user3[user3.coupon_id.notna()][['user_id']]\n",
    "t9['coupon_received'] = 1\n",
    "t9 = t9.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t10 = user3[(user3.date_received!='null')&(user3.date!='null')][['user_id','date_received','date']]\n",
    "t10['user_date_datereceived_gap'] = t10.date + ':' + t10.date_received\n",
    "t10.user_date_datereceived_gap = t10.user_date_datereceived_gap.apply(get_user_date_datereceived_gap)\n",
    "t10 = t10[['user_id','user_date_datereceived_gap']]\n",
    "\n",
    "t11 = t10.groupby('user_id').agg('mean').reset_index()\n",
    "t11.rename(columns={'user_date_datereceived_gap':'avg_user_date_datereceived_gap'},inplace=True)\n",
    "t12 = t10.groupby('user_id').agg('min').reset_index()\n",
    "t12.rename(columns={'user_date_datereceived_gap':'min_user_date_datereceived_gap'},inplace=True)\n",
    "t13 = t10.groupby('user_id').agg('max').reset_index()\n",
    "t13.rename(columns={'user_date_datereceived_gap':'max_user_date_datereceived_gap'},inplace=True)\n",
    "\n",
    "\n",
    "user3_feature = pd.merge(t,t1,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t3,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t4,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t5,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t6,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t7,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t8,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t9,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t11,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t12,on='user_id',how='left')\n",
    "user3_feature = pd.merge(user3_feature,t13,on='user_id',how='left')\n",
    "user3_feature.count_merchant = user3_feature.count_merchant.replace(np.nan,0)\n",
    "user3_feature.buy_use_coupon = user3_feature.buy_use_coupon.replace(np.nan,0)\n",
    "user3_feature['buy_use_coupon_rate'] = user3_feature.buy_use_coupon.astype('float') / user3_feature.buy_total.astype('float')\n",
    "user3_feature['user_coupon_transfer_rate'] = user3_feature.buy_use_coupon.astype('float') / user3_feature.coupon_received.astype('float')\n",
    "user3_feature.buy_total = user3_feature.buy_total.replace(np.nan,0)\n",
    "user3_feature.coupon_received = user3_feature.coupon_received.replace(np.nan,0)\n",
    "user3_feature.to_csv('user3_feature.csv',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#for dataset2\n",
    "user2 = feature2[['user_id','merchant_id','coupon_id','discount_rate','distance','date_received','date']]\n",
    "\n",
    "t = user2[['user_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = user2[user2.date!='null'][['user_id','merchant_id']]\n",
    "t1.drop_duplicates(inplace=True)\n",
    "t1.merchant_id = 1\n",
    "t1 = t1.groupby('user_id').agg('sum').reset_index()\n",
    "t1.rename(columns={'merchant_id':'count_merchant'},inplace=True)\n",
    "\n",
    "t2 = user2[(user2.date!='null')&(user2.coupon_id.notna())][['user_id','distance']]\n",
    "t2.replace(np.nan,-1,inplace=True)\n",
    "t2.distance = t2.distance.astype('int')\n",
    "t2.replace(-1,np.nan,inplace=True)\n",
    "t3 = t2.groupby('user_id').agg('min').reset_index()\n",
    "t3.rename(columns={'distance':'user_min_distance'},inplace=True)\n",
    "\n",
    "t4 = t2.groupby('user_id').agg('max').reset_index()\n",
    "t4.rename(columns={'distance':'user_max_distance'},inplace=True)\n",
    "\n",
    "t5 = t2.groupby('user_id').agg('mean').reset_index()\n",
    "t5.rename(columns={'distance':'user_mean_distance'},inplace=True)\n",
    "\n",
    "t6 = t2.groupby('user_id').agg('median').reset_index()\n",
    "t6.rename(columns={'distance':'user_median_distance'},inplace=True)\n",
    "\n",
    "t7 = user2[(user2.date!='null')&(user2.coupon_id.notna())][['user_id']]\n",
    "t7['buy_use_coupon'] = 1\n",
    "t7 = t7.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t8 = user2[user2.date!='null'][['user_id']]\n",
    "t8['buy_total'] = 1\n",
    "t8 = t8.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t9 = user2[user2.coupon_id.notna()][['user_id']]\n",
    "t9['coupon_received'] = 1\n",
    "t9 = t9.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t10 = user2[(user2.date_received!='null')&(user2.date!='null')][['user_id','date_received','date']]\n",
    "t10['user_date_datereceived_gap'] = t10.date + ':' + t10.date_received\n",
    "t10.user_date_datereceived_gap = t10.user_date_datereceived_gap.apply(get_user_date_datereceived_gap)\n",
    "t10 = t10[['user_id','user_date_datereceived_gap']]\n",
    "\n",
    "t11 = t10.groupby('user_id').agg('mean').reset_index()\n",
    "t11.rename(columns={'user_date_datereceived_gap':'avg_user_date_datereceived_gap'},inplace=True)\n",
    "t12 = t10.groupby('user_id').agg('min').reset_index()\n",
    "t12.rename(columns={'user_date_datereceived_gap':'min_user_date_datereceived_gap'},inplace=True)\n",
    "t13 = t10.groupby('user_id').agg('max').reset_index()\n",
    "t13.rename(columns={'user_date_datereceived_gap':'max_user_date_datereceived_gap'},inplace=True)\n",
    "\n",
    "user2_feature = pd.merge(t,t1,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t3,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t4,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t5,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t6,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t7,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t8,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t9,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t11,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t12,on='user_id',how='left')\n",
    "user2_feature = pd.merge(user2_feature,t13,on='user_id',how='left')\n",
    "user2_feature.count_merchant = user2_feature.count_merchant.replace(np.nan,0)\n",
    "user2_feature.buy_use_coupon = user2_feature.buy_use_coupon.replace(np.nan,0)\n",
    "user2_feature['buy_use_coupon_rate'] = user2_feature.buy_use_coupon.astype('float') / user2_feature.buy_total.astype('float')\n",
    "user2_feature['user_coupon_transfer_rate'] = user2_feature.buy_use_coupon.astype('float') / user2_feature.coupon_received.astype('float')\n",
    "user2_feature.buy_total = user2_feature.buy_total.replace(np.nan,0)\n",
    "user2_feature.coupon_received = user2_feature.coupon_received.replace(np.nan,0)\n",
    "user2_feature.to_csv('user2_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#for dataset1\n",
    "user1 = feature1[['user_id','merchant_id','coupon_id','discount_rate','distance','date_received','date']]\n",
    "\n",
    "t = user1[['user_id']]\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = user1[user1.date!='null'][['user_id','merchant_id']]\n",
    "t1.drop_duplicates(inplace=True)\n",
    "t1.merchant_id = 1\n",
    "t1 = t1.groupby('user_id').agg('sum').reset_index()\n",
    "t1.rename(columns={'merchant_id':'count_merchant'},inplace=True)\n",
    "\n",
    "t2 = user1[(user1.date!='null')&(user1.coupon_id.notna())][['user_id','distance']]\n",
    "t2.replace(np.nan,-1,inplace=True)\n",
    "t2.distance = t2.distance.astype('int')\n",
    "t2.replace(-1,np.nan,inplace=True)\n",
    "t3 = t2.groupby('user_id').agg('min').reset_index()\n",
    "t3.rename(columns={'distance':'user_min_distance'},inplace=True)\n",
    "\n",
    "t4 = t2.groupby('user_id').agg('max').reset_index()\n",
    "t4.rename(columns={'distance':'user_max_distance'},inplace=True)\n",
    "\n",
    "t5 = t2.groupby('user_id').agg('mean').reset_index()\n",
    "t5.rename(columns={'distance':'user_mean_distance'},inplace=True)\n",
    "\n",
    "t6 = t2.groupby('user_id').agg('median').reset_index()\n",
    "t6.rename(columns={'distance':'user_median_distance'},inplace=True)\n",
    "\n",
    "t7 = user1[(user1.date!='null')&(user1.coupon_id.notna())][['user_id']]\n",
    "t7['buy_use_coupon'] = 1\n",
    "t7 = t7.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t8 = user1[user1.date!='null'][['user_id']]\n",
    "t8['buy_total'] = 1\n",
    "t8 = t8.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t9 = user1[user1.coupon_id.notna()][['user_id']]\n",
    "t9['coupon_received'] = 1\n",
    "t9 = t9.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t10 = user1[(user1.date_received!='null')&(user1.date!='null')][['user_id','date_received','date']]\n",
    "t10['user_date_datereceived_gap'] = t10.date + ':' + t10.date_received\n",
    "t10.user_date_datereceived_gap = t10.user_date_datereceived_gap.apply(get_user_date_datereceived_gap)\n",
    "t10 = t10[['user_id','user_date_datereceived_gap']]\n",
    "\n",
    "t11 = t10.groupby('user_id').agg('mean').reset_index()\n",
    "t11.rename(columns={'user_date_datereceived_gap':'avg_user_date_datereceived_gap'},inplace=True)\n",
    "t12 = t10.groupby('user_id').agg('min').reset_index()\n",
    "t12.rename(columns={'user_date_datereceived_gap':'min_user_date_datereceived_gap'},inplace=True)\n",
    "t13 = t10.groupby('user_id').agg('max').reset_index()\n",
    "t13.rename(columns={'user_date_datereceived_gap':'max_user_date_datereceived_gap'},inplace=True)\n",
    "\n",
    "user1_feature = pd.merge(t,t1,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t3,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t4,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t5,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t6,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t7,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t8,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t9,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t11,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t12,on='user_id',how='left')\n",
    "user1_feature = pd.merge(user1_feature,t13,on='user_id',how='left')\n",
    "user1_feature.count_merchant = user1_feature.count_merchant.replace(np.nan,0)\n",
    "user1_feature.buy_use_coupon = user1_feature.buy_use_coupon.replace(np.nan,0)\n",
    "user1_feature['buy_use_coupon_rate'] = user1_feature.buy_use_coupon.astype('float') / user1_feature.buy_total.astype('float')\n",
    "user1_feature['user_coupon_transfer_rate'] = user1_feature.buy_use_coupon.astype('float') / user1_feature.coupon_received.astype('float')\n",
    "user1_feature.buy_total = user1_feature.buy_total.replace(np.nan,0)\n",
    "user1_feature.coupon_received = user1_feature.coupon_received.replace(np.nan,0)\n",
    "user1_feature.to_csv('user1_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#for dataset3\n",
    "all_user_merchant = feature3[['user_id','merchant_id']]\n",
    "all_user_merchant.drop_duplicates(inplace=True)\n",
    "\n",
    "t = feature3[['user_id','merchant_id','date']]\n",
    "t = t[t.date!='null'][['user_id','merchant_id']]\n",
    "t['user_merchant_buy_total'] = 1\n",
    "t = t.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = feature3[['user_id','merchant_id','coupon_id']]\n",
    "t1 = t1[t1.coupon_id.notna()][['user_id','merchant_id']]\n",
    "t1['user_merchant_received'] = 1\n",
    "t1 = t1.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t1.drop_duplicates(inplace=True)\n",
    "\n",
    "t2 = feature3[['user_id','merchant_id','date','date_received']]\n",
    "t2 = t2[(t2.date!='null')&(t2.date_received!='null')][['user_id','merchant_id']]\n",
    "t2['user_merchant_buy_use_coupon'] = 1\n",
    "t2 = t2.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t2.drop_duplicates(inplace=True)\n",
    "\n",
    "t3 = feature3[['user_id','merchant_id']]\n",
    "t3['user_merchant_any'] = 1\n",
    "t3 = t3.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t3.drop_duplicates(inplace=True)\n",
    "\n",
    "t4 = feature3[['user_id','merchant_id','date','coupon_id']]\n",
    "t4 = t4[(t4.date!='null')&(t4.coupon_id.isna())][['user_id','merchant_id']]\n",
    "t4['user_merchant_buy_common'] = 1\n",
    "t4 = t4.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t4.drop_duplicates(inplace=True)\n",
    "\n",
    "user_merchant3 = pd.merge(all_user_merchant,t,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t1,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t2,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t3,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t4,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3.user_merchant_buy_use_coupon = user_merchant3.user_merchant_buy_use_coupon.replace(np.nan,0)\n",
    "user_merchant3.user_merchant_buy_common = user_merchant3.user_merchant_buy_common.replace(np.nan,0)\n",
    "user_merchant3['user_merchant_coupon_transfer_rate'] = user_merchant3.user_merchant_buy_use_coupon.astype('float') / user_merchant3.user_merchant_received.astype('float')\n",
    "user_merchant3['user_merchant_coupon_buy_rate'] = user_merchant3.user_merchant_buy_use_coupon.astype('float') / user_merchant3.user_merchant_buy_total.astype('float')\n",
    "user_merchant3['user_merchant_rate'] = user_merchant3.user_merchant_buy_total.astype('float') / user_merchant3.user_merchant_any.astype('float')\n",
    "user_merchant3['user_merchant_common_buy_rate'] = user_merchant3.user_merchant_buy_common.astype('float') / user_merchant3.user_merchant_buy_total.astype('float')\n",
    "user_merchant3.to_csv('user_merchant3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#for dataset2\n",
    "all_user_merchant = feature2[['user_id','merchant_id']]\n",
    "all_user_merchant.drop_duplicates(inplace=True)\n",
    "\n",
    "t = feature2[['user_id','merchant_id','date']]\n",
    "t = t[t.date!='null'][['user_id','merchant_id']]\n",
    "t['user_merchant_buy_total'] = 1\n",
    "t = t.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = feature2[['user_id','merchant_id','coupon_id']]\n",
    "t1 = t1[t1.coupon_id.notna()][['user_id','merchant_id']]\n",
    "t1['user_merchant_received'] = 1\n",
    "t1 = t1.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t1.drop_duplicates(inplace=True)\n",
    "\n",
    "t2 = feature2[['user_id','merchant_id','date','date_received']]\n",
    "t2 = t2[(t2.date!='null')&(t2.date_received!='null')][['user_id','merchant_id']]\n",
    "t2['user_merchant_buy_use_coupon'] = 1\n",
    "t2 = t2.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t2.drop_duplicates(inplace=True)\n",
    "\n",
    "t3 = feature2[['user_id','merchant_id']]\n",
    "t3['user_merchant_any'] = 1\n",
    "t3 = t3.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t3.drop_duplicates(inplace=True)\n",
    "\n",
    "t4 = feature2[['user_id','merchant_id','date','coupon_id']]\n",
    "t4 = t4[(t4.date!='null')&(t4.coupon_id.isna())][['user_id','merchant_id']]\n",
    "t4['user_merchant_buy_common'] = 1\n",
    "t4 = t4.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t4.drop_duplicates(inplace=True)\n",
    "\n",
    "user_merchant2 = pd.merge(all_user_merchant,t,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t1,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t2,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t3,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t4,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2.user_merchant_buy_use_coupon = user_merchant2.user_merchant_buy_use_coupon.replace(np.nan,0)\n",
    "user_merchant2.user_merchant_buy_common = user_merchant2.user_merchant_buy_common.replace(np.nan,0)\n",
    "user_merchant2['user_merchant_coupon_transfer_rate'] = user_merchant2.user_merchant_buy_use_coupon.astype('float') / user_merchant2.user_merchant_received.astype('float')\n",
    "user_merchant2['user_merchant_coupon_buy_rate'] = user_merchant2.user_merchant_buy_use_coupon.astype('float') / user_merchant2.user_merchant_buy_total.astype('float')\n",
    "user_merchant2['user_merchant_rate'] = user_merchant2.user_merchant_buy_total.astype('float') / user_merchant2.user_merchant_any.astype('float')\n",
    "user_merchant2['user_merchant_common_buy_rate'] = user_merchant2.user_merchant_buy_common.astype('float') / user_merchant2.user_merchant_buy_total.astype('float')\n",
    "user_merchant2.to_csv('user_merchant2.csv',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "all_user_merchant = feature1[['user_id','merchant_id']]\n",
    "all_user_merchant.drop_duplicates(inplace=True)\n",
    "\n",
    "t = feature1[['user_id','merchant_id','date']]\n",
    "t = t[t.date!='null'][['user_id','merchant_id']]\n",
    "t['user_merchant_buy_total'] = 1\n",
    "t = t.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t.drop_duplicates(inplace=True)\n",
    "\n",
    "t1 = feature1[['user_id','merchant_id','coupon_id']]\n",
    "t1 = t1[t1.coupon_id.notna()][['user_id','merchant_id']]\n",
    "t1['user_merchant_received'] = 1\n",
    "t1 = t1.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t1.drop_duplicates(inplace=True)\n",
    "\n",
    "t2 = feature1[['user_id','merchant_id','date','date_received']]\n",
    "t2 = t2[(t2.date!='null')&(t2.date_received!='null')][['user_id','merchant_id']]\n",
    "t2['user_merchant_buy_use_coupon'] = 1\n",
    "t2 = t2.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t2.drop_duplicates(inplace=True)\n",
    "\n",
    "t3 = feature1[['user_id','merchant_id']]\n",
    "t3['user_merchant_any'] = 1\n",
    "t3 = t3.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t3.drop_duplicates(inplace=True)\n",
    "\n",
    "t4 = feature1[['user_id','merchant_id','date','coupon_id']]\n",
    "t4 = t4[(t4.date!='null')&(t4.coupon_id.isna())][['user_id','merchant_id']]\n",
    "t4['user_merchant_buy_common'] = 1\n",
    "t4 = t4.groupby(['user_id','merchant_id']).agg('sum').reset_index()\n",
    "t4.drop_duplicates(inplace=True)\n",
    "\n",
    "user_merchant1 = pd.merge(all_user_merchant,t,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t1,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t2,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t3,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t4,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1.user_merchant_buy_use_coupon = user_merchant1.user_merchant_buy_use_coupon.replace(np.nan,0)\n",
    "user_merchant1.user_merchant_buy_common = user_merchant1.user_merchant_buy_common.replace(np.nan,0)\n",
    "user_merchant1['user_merchant_coupon_transfer_rate'] = user_merchant1.user_merchant_buy_use_coupon.astype('float') / user_merchant1.user_merchant_received.astype('float')\n",
    "user_merchant1['user_merchant_coupon_buy_rate'] = user_merchant1.user_merchant_buy_use_coupon.astype('float') / user_merchant1.user_merchant_buy_total.astype('float')\n",
    "user_merchant1['user_merchant_rate'] = user_merchant1.user_merchant_buy_total.astype('float') / user_merchant1.user_merchant_any.astype('float')\n",
    "user_merchant1['user_merchant_common_buy_rate'] = user_merchant1.user_merchant_buy_common.astype('float') / user_merchant1.user_merchant_buy_total.astype('float')\n",
    "user_merchant1.to_csv('user_merchant1.csv',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112803, 52)\n"
     ]
    }
   ],
   "source": [
    "coupon3 = pd.read_csv('coupon3_feature.csv')\n",
    "merchant3 = pd.read_csv('merchant3_feature.csv')\n",
    "user3 = pd.read_csv('user3_feature.csv')\n",
    "user_merchant3 = pd.read_csv('user_merchant3.csv')\n",
    "other_feature3 = pd.read_csv('other_feature3.csv')\n",
    "dataset3 = pd.merge(coupon3,merchant3,on='merchant_id',how='left')\n",
    "dataset3 = pd.merge(dataset3,user3,on='user_id',how='left')\n",
    "dataset3 = pd.merge(dataset3,user_merchant3,on=['user_id','merchant_id'],how='left')\n",
    "dataset3 = pd.merge(dataset3,other_feature3,on=['user_id','coupon_id','date_received'],how='left')\n",
    "dataset3.drop_duplicates(inplace=True)\n",
    "print(dataset3.shape)\n",
    "\n",
    "dataset3.user_merchant_buy_total = dataset3.user_merchant_buy_total.replace(np.nan,0)\n",
    "dataset3.user_merchant_any = dataset3.user_merchant_any.replace(np.nan,0)\n",
    "dataset3.user_merchant_received = dataset3.user_merchant_received.replace(np.nan,0)\n",
    "dataset3['is_weekend'] = dataset3.day_of_week.apply(lambda x:1 if x in (6,7) else 0)\n",
    "weekday_dummies = pd.get_dummies(dataset3.day_of_week)\n",
    "weekday_dummies.columns = ['weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset3 = pd.concat([dataset3,weekday_dummies],axis=1)\n",
    "dataset3.drop(['merchant_id','day_of_week','coupon_count'],axis=1,inplace=True)\n",
    "dataset3 = dataset3.replace('null',np.nan)\n",
    "dataset3.to_csv('dataset3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_label(s):\n",
    "#     s = s.split(':')\n",
    "#     if s[0] == 'nan':\n",
    "#         return 0\n",
    "#     if s[1]!= 'nan': \n",
    "#         if (date(int(s[0][0:4]),int(s[0][4:6]),int(s[0][6:8]))-date(int(s[1][0:4]),int(s[1][4:6]),int(s[1][6:8]))).days<=15:\n",
    "#             return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "def label(row):\n",
    "    if row['date_received'] == 'null':\n",
    "        return -1\n",
    "    if row['date'] != 'null':\n",
    "        td = pd.to_datetime(row['date'], format='%Y%m%d') -  pd.to_datetime(row['date_received'], format='%Y%m%d')\n",
    "        if td <= pd.Timedelta(15, 'D'):\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253670, 53)\n",
      "0           NaN\n",
      "1           NaN\n",
      "2    20160613.0\n",
      "3           NaN\n",
      "4           NaN\n",
      "Name: date, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "coupon2 = pd.read_csv('coupon2_feature.csv')\n",
    "merchant2 = pd.read_csv('merchant2_feature.csv')\n",
    "user2 = pd.read_csv('user2_feature.csv')\n",
    "user_merchant2 = pd.read_csv('user_merchant2.csv')\n",
    "other_feature2 = pd.read_csv('other_feature2.csv')\n",
    "dataset2 = pd.merge(coupon2,merchant2,on='merchant_id',how='left')\n",
    "dataset2 = pd.merge(dataset2,user2,on='user_id',how='left')\n",
    "dataset2 = pd.merge(dataset2,user_merchant2,on=['user_id','merchant_id'],how='left')\n",
    "dataset2 = pd.merge(dataset2,other_feature2,on=['user_id','coupon_id','date_received'],how='left')\n",
    "dataset2.drop_duplicates(inplace=True)\n",
    "print(dataset2.shape)\n",
    "print(dataset2.date.head(5))\n",
    "\n",
    "dataset2.user_merchant_buy_total = dataset2.user_merchant_buy_total.replace(np.nan,0)\n",
    "dataset2.user_merchant_any = dataset2.user_merchant_any.replace(np.nan,0)\n",
    "dataset2.user_merchant_received = dataset2.user_merchant_received.replace(np.nan,0)\n",
    "dataset2['is_weekend'] = dataset2.day_of_week.apply(lambda x:1 if x in (6,7) else 0)\n",
    "weekday_dummies = pd.get_dummies(dataset2.day_of_week)\n",
    "weekday_dummies.columns = ['weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset2 = pd.concat([dataset2,weekday_dummies],axis=1)\n",
    "dataset2['label'] = dataset2.apply(label,axis=1)\n",
    "dataset2.drop(['merchant_id','day_of_week','date','date_received','coupon_count'],axis=1,inplace=True) #这里我取走coupon_id\n",
    "dataset2 = dataset2.replace('nan',np.nan)\n",
    "dataset2.to_csv('dataset2.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130957, 53)\n"
     ]
    }
   ],
   "source": [
    "coupon1 = pd.read_csv('coupon1_feature.csv')\n",
    "merchant1 = pd.read_csv('merchant1_feature.csv')\n",
    "user1 = pd.read_csv('user1_feature.csv')\n",
    "user_merchant1 = pd.read_csv('user_merchant1.csv')\n",
    "other_feature1 = pd.read_csv('other_feature1.csv')\n",
    "dataset1 = pd.merge(coupon1,merchant1,on='merchant_id',how='left')\n",
    "dataset1 = pd.merge(dataset1,user1,on='user_id',how='left')\n",
    "dataset1 = pd.merge(dataset1,user_merchant1,on=['user_id','merchant_id'],how='left')\n",
    "dataset1 = pd.merge(dataset1,other_feature1,on=['user_id','coupon_id','date_received'],how='left')\n",
    "dataset1.drop_duplicates(inplace=True)\n",
    "print(dataset1.shape)\n",
    "\n",
    "dataset1.user_merchant_buy_total = dataset1.user_merchant_buy_total.replace(np.nan,0)\n",
    "dataset1.user_merchant_any = dataset1.user_merchant_any.replace(np.nan,0)\n",
    "dataset1.user_merchant_received = dataset1.user_merchant_received.replace(np.nan,0)\n",
    "dataset1['is_weekend'] = dataset1.day_of_week.apply(lambda x:1 if x in (6,7) else 0)\n",
    "weekday_dummies = pd.get_dummies(dataset1.day_of_week)\n",
    "weekday_dummies.columns = ['weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset1 = pd.concat([dataset1,weekday_dummies],axis=1)\n",
    "dataset1['label'] = dataset1.apply(label,axis=1)\n",
    "# dataset1['label'] = dataset1.date.astype('str') + ':' +  dataset1.date_received.astype('str')\n",
    "# dataset1.label = dataset1.label.apply(get_label)\n",
    "dataset1.drop(['merchant_id','day_of_week','date','date_received','coupon_id','coupon_count'],axis=1,inplace=True)\n",
    "dataset1 = dataset1.replace('nan',np.nan)\n",
    "dataset1.to_csv('dataset1.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129151, 52) (253380, 52) (112803, 52) (382531, 52)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = pd.read_csv('dataset1.csv')\n",
    "dataset1.label.replace(-1,0,inplace=True)\n",
    "dataset2 = pd.read_csv('dataset2.csv')\n",
    "dataset2.label.replace(-1,0,inplace=True)\n",
    "dataset3 = pd.read_csv('dataset3.csv')\n",
    "\n",
    "dataset1.drop_duplicates(inplace=True)\n",
    "dataset2.drop_duplicates(inplace=True)\n",
    "dataset3.drop_duplicates(inplace=True)\n",
    "\n",
    "dataset12 = pd.concat([dataset1,dataset2],axis=0)\n",
    "\n",
    "dataset1_y = dataset1.label\n",
    "dataset1_x = dataset1.drop(['user_id','label','day_gap_before','day_gap_after'],axis=1)  # 'day_gap_before','day_gap_after' cause overfitting, 0.77\n",
    "# dataset2_y = dataset2.label\n",
    "dataset2_y = dataset2[['coupon_id','label']]\n",
    "dataset2_x = dataset2.drop(['user_id','coupon_id','label','day_gap_before','day_gap_after'],axis=1)\n",
    "dataset12_y = dataset12.label\n",
    "dataset12_x = dataset12.drop(['user_id','label','coupon_id','day_gap_before','day_gap_after'],axis=1)\n",
    "dataset3_preds = dataset3[['user_id','coupon_id','date_received']]\n",
    "dataset3_x = dataset3.drop(['user_id','coupon_id','date_received','day_gap_before','day_gap_after'],axis=1)\n",
    "\n",
    "print(dataset1_x.shape,dataset2_x.shape,dataset3_x.shape,dataset12_x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里试一下用贝叶斯优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | num_le... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8808  \u001b[0m | \u001b[0m 3.958   \u001b[0m | \u001b[0m 104.7   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8852  \u001b[0m | \u001b[95m 5.189   \u001b[0m | \u001b[95m 124.2   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8841  \u001b[0m | \u001b[0m 6.9     \u001b[0m | \u001b[0m 62.71   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8843  \u001b[0m | \u001b[0m 4.382   \u001b[0m | \u001b[0m 126.2   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.882   \u001b[0m | \u001b[0m 7.791   \u001b[0m | \u001b[0m 135.1   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8808  \u001b[0m | \u001b[0m 3.054   \u001b[0m | \u001b[0m 30.01   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.882   \u001b[0m | \u001b[0m 7.961   \u001b[0m | \u001b[0m 150.0   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8808  \u001b[0m | \u001b[0m 3.096   \u001b[0m | \u001b[0m 149.9   \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.8857  \u001b[0m | \u001b[95m 7.892   \u001b[0m | \u001b[95m 30.02   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8857  \u001b[0m | \u001b[0m 7.96    \u001b[0m | \u001b[0m 30.01   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8857  \u001b[0m | \u001b[0m 7.998   \u001b[0m | \u001b[0m 30.19   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8857  \u001b[0m | \u001b[0m 7.977   \u001b[0m | \u001b[0m 30.18   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8857  \u001b[0m | \u001b[0m 7.999   \u001b[0m | \u001b[0m 30.12   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8857  \u001b[0m | \u001b[0m 7.994   \u001b[0m | \u001b[0m 30.16   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8857  \u001b[0m | \u001b[0m 7.997   \u001b[0m | \u001b[0m 30.01   \u001b[0m |\n",
      "=================================================\n",
      "Final result: {'target': 0.8856833392121676, 'params': {'max_depth': 7.891796615348076, 'num_leaves': 30.01776499406042}}\n"
     ]
    }
   ],
   "source": [
    "def lgb_cv(max_depth,num_leaves,data,targets):\n",
    "    estimator = lgb.LGBMClassifier(\n",
    "                    learning_rate = 0.01,\n",
    "                    boosting_type = 'gbdt',\n",
    "                    metric = 'auc',\n",
    "                    objective = 'binary',\n",
    "                    max_bin = 100,\n",
    "                    max_depth = max_depth,\n",
    "                    sub_feature = 0.7,\n",
    "                    num_leaves = num_leaves,\n",
    "                    colsample_bytree = 0.7,\n",
    "                    n_estimators = 5000,\n",
    "                    early_stop = 50,\n",
    "                    verbose = -1)\n",
    "    cval = cross_val_score(estimator, data, targets,\n",
    "                           scoring='roc_auc', cv=3)\n",
    "    return cval.mean()\n",
    "def optimize_lgb(data,targets):\n",
    "    def lgb_crossval(max_depth,num_leaves):\n",
    "        return lgb_cv(\n",
    "            max_depth = int(max_depth),\n",
    "            num_leaves = int(num_leaves),\n",
    "            data = data,\n",
    "            targets = targets\n",
    "        )\n",
    "    optimizer = BayesianOptimization(\n",
    "        f = lgb_crossval,\n",
    "        pbounds = {\n",
    "                'max_depth':(3,8),\n",
    "                'num_leaves':(30,150),\n",
    "        },\n",
    "        random_state=1234,\n",
    "        verbose= 2   \n",
    "    )\n",
    "    optimizer.maximize(n_iter=10)\n",
    "    print(\"Final result:\", optimizer.max)\n",
    "optimize_lgb(dataset2_x,dataset2_y.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面的是使用gridsearch的方式查找lgb的最优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed: 82.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931004814902518\n",
      "{'en__learning_rate': 0.01, 'en__num_leaves': 32}\n"
     ]
    }
   ],
   "source": [
    "# def check_model(train_x, train_y):\n",
    "    \n",
    "#     classifier = lambda: lgb.LGBMClassifier(\n",
    "#                     learning_rate = 0.05,\n",
    "#                     boosting_type = 'gbdt',\n",
    "#                     metric = 'auc',\n",
    "#                     objective = 'binary',\n",
    "#                     max_bin = 100,\n",
    "#                     max_depth = 6,\n",
    "#                     sub_feature = 0.7,\n",
    "#                     num_leaves = 6,\n",
    "#                     colsample_bytree = 0.7,\n",
    "#                     n_estimators = 5000,\n",
    "#                     early_stop = 50,\n",
    "#                     verbose = -1)\n",
    "\n",
    "#     model = Pipeline(steps=[\n",
    "#         ('en', classifier())\n",
    "#     ])\n",
    "\n",
    "#     parameters = {\n",
    "#         'en__num_leaves': [ 6, 16, 32],\n",
    "#         'en__learning_rate': [ 0.001, 0.01]\n",
    "#     }\n",
    "\n",
    "#     folder = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    \n",
    "#     grid_search = GridSearchCV(\n",
    "#         model, \n",
    "#         parameters, \n",
    "#         cv=folder, \n",
    "#         n_jobs=-1, \n",
    "#         verbose=1)\n",
    "#     grid_search = grid_search.fit(train_x, \n",
    "#                                   train_y)\n",
    "    \n",
    "#     return grid_search\n",
    "# model = check_model(dataset2_x, dataset2_y.label)\n",
    "# print(model.best_score_)\n",
    "# print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里试一下stacking3个模型(构建函数的方式好像有问题先注释了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_model1(data, predictors):\n",
    "    \n",
    "#     classifier = lambda: SGDClassifier(\n",
    "#         loss='log', \n",
    "#         penalty='elasticnet', \n",
    "#         fit_intercept=True, \n",
    "#         max_iter=100, \n",
    "#         shuffle=True, \n",
    "#         n_jobs=1,\n",
    "#         class_weight=None)\n",
    "\n",
    "#     model = Pipeline(steps=[\n",
    "#         ('ss', StandardScaler()),\n",
    "#         ('en', classifier())\n",
    "#     ])\n",
    "\n",
    "#     parameters = {\n",
    "#         'en__alpha': [0.001],\n",
    "#         'en__l1_ratio': [0.01]\n",
    "#     }\n",
    "\n",
    "#     folder = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    \n",
    "#     grid_search = GridSearchCV(\n",
    "#         model, \n",
    "#         parameters, \n",
    "#         cv=folder, \n",
    "#         n_jobs=-1, \n",
    "#         verbose=1)\n",
    "#     grid_search = grid_search.fit(data[predictors], \n",
    "#                                   data['label'])\n",
    "    \n",
    "#     return grid_search\n",
    "\n",
    "# def check_model2(train_x, train_y):\n",
    "    \n",
    "#     classifier = lambda: lgb.LGBMClassifier(\n",
    "#                     learning_rate = 0.05,\n",
    "#                     boosting_type = 'gbdt',\n",
    "#                     metric = 'auc',\n",
    "#                     objective = 'binary',\n",
    "#                     max_bin = 100,\n",
    "#                     max_depth = 6,\n",
    "#                     sub_feature = 0.7,\n",
    "#                     num_leaves = 6,\n",
    "#                     colsample_bytree = 0.7,\n",
    "#                     n_estimators = 5000,\n",
    "#                     early_stop = 50,\n",
    "#                     verbose = -1)\n",
    "\n",
    "#     model = Pipeline(steps=[\n",
    "#         ('en', classifier())\n",
    "#     ])\n",
    "\n",
    "#     parameters = {\n",
    "#         'en__num_leaves': [32],\n",
    "#         'en__learning_rate': [0.01]\n",
    "#     }\n",
    "\n",
    "#     folder = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    \n",
    "#     grid_search = GridSearchCV(\n",
    "#         model, \n",
    "#         parameters, \n",
    "#         cv=folder, \n",
    "#         n_jobs=-1, \n",
    "#         verbose=1)\n",
    "#     grid_search = grid_search.fit(train_x, \n",
    "#                                   train_y)\n",
    "    \n",
    "#     return grid_search\n",
    "\n",
    "# def check_model3(dataset):\n",
    "#     params={'booster':'gbtree',\n",
    "# \t    'objective': 'rank:pairwise',\n",
    "# \t    'eval_metric':'auc',\n",
    "# \t    'gamma':0.1,\n",
    "# \t    'min_child_weight':1.1,\n",
    "# \t    'max_depth':5,\n",
    "# \t    'lambda':10,\n",
    "# \t    'subsample':0.7,\n",
    "# \t    'colsample_bytree':0.7,\n",
    "# \t    'colsample_bylevel':0.7,\n",
    "# \t    'eta': 0.01,\n",
    "# \t    'tree_method':'exact',\n",
    "# \t    'seed':0,\n",
    "# \t    'nthread':12\n",
    "# \t    }\n",
    "#     watchlist = [(dataset,'train')]\n",
    "#     model = xgb.train(params,dataset,num_boost_round=3500,evals=watchlist)\n",
    "#     return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StackingClassifier' object has no attribute 'predit_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-6c426405a43a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m                           meta_classifier=lr)\n\u001b[0;32m     42\u001b[0m \u001b[0msclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset2_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset2_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mpredit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredit_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset2_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AUC\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset2_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StackingClassifier' object has no attribute 'predit_proba'"
     ]
    }
   ],
   "source": [
    "# clf1 = SGDClassifier(\n",
    "#         loss='log', \n",
    "#         penalty='elasticnet', \n",
    "#         fit_intercept=True, \n",
    "#         max_iter=100, \n",
    "#         shuffle=True, \n",
    "#         n_jobs=1,\n",
    "#         class_weight=None,\n",
    "#         alpha=0.001,\n",
    "#         l1_ratio= 0.01)\n",
    "clf2 = lgb.LGBMClassifier(\n",
    "                    learning_rate = 0.01,\n",
    "                    boosting_type = 'gbdt',\n",
    "                    metric = 'auc',\n",
    "                    objective = 'binary',\n",
    "                    max_bin = 100,\n",
    "                    max_depth = 6,\n",
    "                    sub_feature = 0.7,\n",
    "                    colsample_bytree = 0.7,\n",
    "                    n_estimators = 5000,\n",
    "                    early_stop = 50,\n",
    "                    verbose = -1,\n",
    "                    num_leaves= 32\n",
    ")\n",
    "clf3 = xgb.XGBClassifier(\n",
    "                    booster='gbtree',\n",
    "                    objective= 'rank:pairwise',\n",
    "                    eval_metric='auc',\n",
    "                    gamma=0.1,\n",
    "                    min_child_weight=1.1,\n",
    "                    max_depth=5,\n",
    "                    subsample=0.7,\n",
    "                    colsample_bytree=0.7,\n",
    "                    colsample_bylevel=0.7,\n",
    "                    eta= 0.01,\n",
    "                    tree_method='exact',\n",
    "                    seed=0,\n",
    "                    nthread=12 )\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "sclf.fit(dataset2_x, dataset2_y.label)\n",
    "predit = sclf.predit_proba(dataset2_x)[:,1]\n",
    "print(\"AUC\",roc_auc_score(dataset2_y.label,predit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'collections.OrderedDict'>, {})\n"
     ]
    }
   ],
   "source": [
    "model2 = lgb.LGBMClassifier(\n",
    "                    learning_rate = 0.05,\n",
    "                    boosting_type = 'gbdt',\n",
    "                    metric = 'auc',\n",
    "                    objective = 'binary',\n",
    "                    max_bin = 100,\n",
    "                    max_depth = 8,\n",
    "                    sub_feature = 0.7,\n",
    "                    num_leaves = 30,\n",
    "                    colsample_bytree = 0.7,\n",
    "                    n_estimators = 5000,\n",
    "                    early_stop = 50,\n",
    "                    verbose = -1)\n",
    "model2.fit(dataset12_x, dataset12_y)\n",
    "print(model2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5933791895673571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset2_y['pred_prob'] = model.predict_proba(dataset2_x)[:,1]\n",
    "dataset2group = dataset2_y.groupby(['coupon_id'])\n",
    "aucs = []\n",
    "for i in dataset2group:\n",
    "    tmpdf = i[1]\n",
    "    if len(tmpdf['label'].unique())!=2:\n",
    "        continue\n",
    "        \n",
    "    fpr,tpr,thresholds = roc_curve(tmpdf['label'].fillna(0.0),tmpdf['pred_prob'].fillna(0),pos_label=1)\n",
    "    aucs.append(auc(fpr,tpr))\n",
    "print(np.average(aucs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dataset3_preds['probability'] = model.predict_proba(dataset3_x)[:,1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>coupon_id</th>\n",
       "      <th>date_received</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4129537</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160712</td>\n",
       "      <td>0.098909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6949378</td>\n",
       "      <td>3429</td>\n",
       "      <td>20160706</td>\n",
       "      <td>0.093892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2166529</td>\n",
       "      <td>6928</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.010148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2166529</td>\n",
       "      <td>1808</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.010085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6172162</td>\n",
       "      <td>6500</td>\n",
       "      <td>20160708</td>\n",
       "      <td>0.041029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4005121</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160706</td>\n",
       "      <td>0.173511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4347394</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160716</td>\n",
       "      <td>0.084449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3094273</td>\n",
       "      <td>13602</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.101225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5139970</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160729</td>\n",
       "      <td>0.029275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3237121</td>\n",
       "      <td>13602</td>\n",
       "      <td>20160703</td>\n",
       "      <td>0.048527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6224386</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160716</td>\n",
       "      <td>0.043035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6488578</td>\n",
       "      <td>13602</td>\n",
       "      <td>20160712</td>\n",
       "      <td>0.317193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4164865</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160703</td>\n",
       "      <td>0.099085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4164865</td>\n",
       "      <td>8059</td>\n",
       "      <td>20160706</td>\n",
       "      <td>0.029030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5468674</td>\n",
       "      <td>9983</td>\n",
       "      <td>20160713</td>\n",
       "      <td>0.161007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6258178</td>\n",
       "      <td>9144</td>\n",
       "      <td>20160706</td>\n",
       "      <td>0.094830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3659521</td>\n",
       "      <td>7341</td>\n",
       "      <td>20160727</td>\n",
       "      <td>0.038263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3659521</td>\n",
       "      <td>13181</td>\n",
       "      <td>20160717</td>\n",
       "      <td>0.030875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3659521</td>\n",
       "      <td>13602</td>\n",
       "      <td>20160718</td>\n",
       "      <td>0.160015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7333378</td>\n",
       "      <td>13602</td>\n",
       "      <td>20160704</td>\n",
       "      <td>0.125819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  coupon_id  date_received  probability\n",
       "0   4129537       9983       20160712     0.098909\n",
       "1   6949378       3429       20160706     0.093892\n",
       "2   2166529       6928       20160727     0.010148\n",
       "3   2166529       1808       20160727     0.010085\n",
       "4   6172162       6500       20160708     0.041029\n",
       "5   4005121       9983       20160706     0.173511\n",
       "6   4347394       9983       20160716     0.084449\n",
       "7   3094273      13602       20160727     0.101225\n",
       "8   5139970       9983       20160729     0.029275\n",
       "9   3237121      13602       20160703     0.048527\n",
       "10  6224386       9983       20160716     0.043035\n",
       "11  6488578      13602       20160712     0.317193\n",
       "12  4164865       9983       20160703     0.099085\n",
       "13  4164865       8059       20160706     0.029030\n",
       "14  5468674       9983       20160713     0.161007\n",
       "15  6258178       9144       20160706     0.094830\n",
       "16  3659521       7341       20160727     0.038263\n",
       "17  3659521      13181       20160717     0.030875\n",
       "18  3659521      13602       20160718     0.160015\n",
       "19  7333378      13602       20160704     0.125819"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3_preds.to_csv('submit7.csv',index=False, header=False) \n",
    "dataset3_preds.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里尝试用adaboost看看效果(看来不行，他处理不了NAN值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adaboost(train_x,train_y,valid_x,valid_y):\n",
    "#     model = AdaBoostClassifier().fit(train_x,train_y)\n",
    "#     valid_y['pred_prob'] = model.predict_proba(vaiid_x[:,1])\n",
    "#     validgroup = valid_y.groupby(['coupon_id'])\n",
    "#     aucs = []\n",
    "#     for i in validgroup:\n",
    "#         tmpdf = i[1]\n",
    "#         if len(tmpdf['label'].unique())!=2:\n",
    "#             continue\n",
    "\n",
    "#         fpr,tpr,thresholds = roc_curve(tmpdf['label'],tmpdf['pred_prob'],pos_label=1)\n",
    "#         aucs.append(auc(fpr,tpr))\n",
    "#     print(np.average(aucs))\n",
    "#     return model\n",
    "# model_ada = adaboost(dataset12_x,dataset12_y,dataset1_x,dataset2_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是xgbboost的模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset1 = xgb.DMatrix(dataset1_x,label=dataset1_y)\n",
    "# dataset2 = xgb.DMatrix(dataset2_x,label=dataset2_y.label)\n",
    "# dataset12 = xgb.DMatrix(dataset12_x,label=dataset12_y)\n",
    "# dataset3_x = dataset3_x[dataset12_x.columns]\n",
    "# dataset3 = xgb.DMatrix(dataset3_x)\n",
    "\n",
    "params={'booster':'gbtree',\n",
    "\t    'objective': 'rank:pairwise',\n",
    "\t    'eval_metric':'auc',\n",
    "\t    'gamma':0.1,\n",
    "\t    'min_child_weight':1.1,\n",
    "\t    'max_depth':5,\n",
    "\t    'lambda':10,\n",
    "\t    'subsample':0.7,\n",
    "\t    'colsample_bytree':0.7,\n",
    "\t    'colsample_bylevel':0.7,\n",
    "\t    'eta': 0.01,\n",
    "\t    'tree_method':'exact',\n",
    "\t    'seed':0,\n",
    "\t    'nthread':12\n",
    "\t    }\n",
    "\n",
    "#train on dataset1, evaluate on dataset2\n",
    "#watchlist = [(dataset1,'train'),(dataset2,'val')]\n",
    "#model = xgb.train(params,dataset1,num_boost_round=3000,evals=watchlist,early_stopping_rounds=300)\n",
    "\n",
    "watchlist = [(dataset12,'train')]\n",
    "model = xgb.train(params,dataset12,num_boost_round=3500,evals=watchlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict test set\n",
    "# dataset3_preds['probability'] = model.predict(dataset3)\n",
    "# dataset3_preds.probability = MinMaxScaler().fit_transform(dataset3_preds.probability.values.reshape(-1, 1))\n",
    "# dataset3_preds.sort_values(by=['coupon_id','probability'],inplace=True)\n",
    "# dataset3_preds.to_csv(\"xgb_preds.csv\",index=None,header=None)\n",
    "# print(dataset3_preds.describe())\n",
    "    \n",
    "# #save feature score\n",
    "# feature_score = model.get_fscore()\n",
    "# feature_score = sorted(feature_score.items(), key=lambda x:x[1],reverse=True)\n",
    "# fs = []\n",
    "# for (key,value) in feature_score:\n",
    "#     fs.append(\"{0},{1}\\n\".format(key,value))\n",
    "    \n",
    "# with open('xgb_feature_score.csv','w') as f:\n",
    "#     f.writelines(\"feature,score\\n\")\n",
    "#     f.writelines(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'brier_score_loss',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'mutual_info_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# off_train.date = off_train.date.replace('null',np.nan)\n",
    "# dataset12_x.head(20)\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
